{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Analyse des sentiements liées aux entitées\n",
    "On souhaite ici compter le nombre de messages décrit comme \"positif\" et \"négatif\" comptenant une entitée\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import TFCamembertForSequenceClassification\n",
    "import transformers.models.camembert.tokenization_camembert as tk"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ]
  },
  {
   "source": [
    "### Chargement des messages et des entitées"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./messages/BDE_8223.json', encoding=\"utf8\") as f:\n",
    "  messages = json.load(f)\n",
    "\n",
    "with open('./entities/entities_final.json', encoding=\"utf8\") as f:\n",
    "  entities = json.load(f)"
   ]
  },
  {
   "source": [
    "### Récuparation de l'encodeur pour notre modèle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tk.CamembertTokenizer.from_pretrained(\"jplu/tf-camembert-base\",do_lower_case=True)\n",
    "assert tokenizer != None\n",
    "\n",
    "def encode_msg(messages, tokenizer = tokenizer, max_length=80):\n",
    "    token_ids = np.zeros(shape=(len(messages), max_length),\n",
    "                         dtype=np.int32)\n",
    "    for i, msg in enumerate(messages):\n",
    "        encoded = tokenizer.encode(msg, max_length=max_length)\n",
    "        token_ids[i, 0:len(encoded)] = encoded\n",
    "    attention_mask = (token_ids != 0).astype(np.int32)\n",
    "    return {\"input_ids\": token_ids, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "source": [
    "### Chargement de notre modèle fine-tuned"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFCamembertForSequenceClassification.\n",
      "\n",
      "Some layers of TFCamembertForSequenceClassification were not initialized from the model checkpoint at jplu/tf-camembert-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFCamembertForSequenceClassification.from_pretrained(\"jplu/tf-camembert-base\")\n",
    "model.load_weights(\"./models_weights/f179_count8000_epo20_batch4.h5\")"
   ]
  },
  {
   "source": [
    "### Pré-traitement de notre jeu de donnée\n",
    "On écarte les messages comptenant des liens, des gif et des enregistrement vocaux. On ne garde pas non plus les messages de moins de 2 caractères ainsi que les messages à plus de 120 caractères car le modèle a été entrainé sur des messages courts.\n",
    "\n",
    "On format aussi les données en ne gardant que le contenu et l'auteur du messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6972 messages gardés\n"
     ]
    }
   ],
   "source": [
    "# entrée (messages) { who:.., what:.., when:.., feedback:.., whatType:..},{..},{..},..\n",
    "# sortie (messages_keep) [[what, who],[],..]\n",
    "\n",
    "messages_keep = [[m['what'],m['who']] for m in messages if m['whatType'] == \"Texte\"]\n",
    "\n",
    "df = pd.DataFrame(messages_keep,columns=['messages','auteur'])\n",
    "print(f\"{len(df)} messages gardés\")"
   ]
  },
  {
   "source": [
    "### Prédiction des sentiements avec le modèle fine tuned"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001F4F926AFA0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001F4F926AFA0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From C:\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFRobertaClassificationHead.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaClassificationHead object at 0x000001F5058BDB50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmppmiz0b3z.py, line 10)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFRobertaClassificationHead.call of <transformers.models.roberta.modeling_tf_roberta.TFRobertaClassificationHead object at 0x000001F5058BDB50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: invalid syntax (tmppmiz0b3z.py, line 10)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            messages   auteur  sentiment\n",
       "0  Mais elle a même pas présenté ce que c'était l...  Fanny-T          0\n",
       "1                         Bah tu fais ce que tu veux  Olivier          1\n",
       "2                Elle a dit qu’on fait ce qu’on veut       Al          1\n",
       "3                                                Meh  Fanny-T          1\n",
       "4  Mais la vous avez tous des projets ou vous dem...       Al          0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>messages</th>\n      <th>auteur</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Mais elle a même pas présenté ce que c'était l...</td>\n      <td>Fanny-T</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bah tu fais ce que tu veux</td>\n      <td>Olivier</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Elle a dit qu’on fait ce qu’on veut</td>\n      <td>Al</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Meh</td>\n      <td>Fanny-T</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Mais la vous avez tous des projets ou vous dem...</td>\n      <td>Al</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# sortie [[what,who,sentiment],..]\n",
    "\n",
    "messages_array = df.iloc[:,0].values\n",
    "encoded_messages = encode_msg(messages_array)\n",
    "\n",
    "scores = model.predict(encoded_messages)\n",
    "sent_pred = np.argmax(scores['logits'], axis=1)\n",
    "df['sentiment'] = sent_pred\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "63.4% messages positifs (4422)\n36.6% messages négatifs (2550)\n"
     ]
    }
   ],
   "source": [
    "mneg = df.iloc[:,2].value_counts(0)[0]\n",
    "mpos = df.iloc[:,2].value_counts(0)[1]\n",
    "tot = mpos+mneg\n",
    "print(f\"{round(100 * mpos/tot,1)}% messages positifs ({mpos})\")\n",
    "print(f\"{round(100 * mneg/tot,1)}% messages négatifs ({mneg})\")"
   ]
  },
  {
   "source": [
    "### Coloration des entitées\n",
    "Association pour chaque entitées(groupe de mot) du nombre de messagse positif et négatif comptenant un des mots liées à l'entitée"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sortie [{entities: [n_positif,n_negatif,diff] }]\n",
    "\n",
    "def containOneOf(message,elements):\n",
    "    for e in elements:\n",
    "        if e in message.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "entities_with_sentiment = {}\n",
    "\n",
    "n_sans_entity = 0\n",
    "for row in df.to_numpy():\n",
    "    sans_entity = True\n",
    "    for ent in entities:\n",
    "        if containOneOf(row[0],entities[ent]):\n",
    "            arr = np.array([0,0,0])\n",
    "            arr[row[2]] = 1\n",
    "            if ent not in entities_with_sentiment:\n",
    "                entities_with_sentiment[ent] = arr.tolist()\n",
    "            else:\n",
    "                entities_with_sentiment[ent] = (np.array(arr)+np.array(entities_with_sentiment[ent])).tolist()\n",
    "            sans_entity = False\n",
    "    if sans_entity:\n",
    "        n_sans_entity += 1\n",
    "\n",
    "# ajout de la différence sentiement pos - neg\n",
    "for ent in entities_with_sentiment:\n",
    "    entities_with_sentiment[ent][2] = entities_with_sentiment[ent][1] - entities_with_sentiment[ent][0] \n",
    "\n",
    "# sauvegarde de l'analyse\n",
    "with open('analyse.json', 'w', encoding=\"utf8\") as fout:\n",
    "    json.dump(entities_with_sentiment, fout, ensure_ascii=False)"
   ]
  },
  {
   "source": [
    "### Convertion du dict en DataFrame pour visualiser le résultat"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       entity  negatif  positif  diff\n",
       "11      julie       39      138    99\n",
       "42     soirée       19       56    37\n",
       "12   juliette       13       43    30\n",
       "39        wec       17       45    28\n",
       "3      albane        4       29    25\n",
       "..        ...      ...      ...   ...\n",
       "97        paf       10        7    -3\n",
       "67       homo        3        0    -3\n",
       "17    kermess        4        1    -3\n",
       "22       noël        7        4    -3\n",
       "102        pc        5        1    -4\n",
       "\n",
       "[143 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entity</th>\n      <th>negatif</th>\n      <th>positif</th>\n      <th>diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11</th>\n      <td>julie</td>\n      <td>39</td>\n      <td>138</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>soirée</td>\n      <td>19</td>\n      <td>56</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>juliette</td>\n      <td>13</td>\n      <td>43</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>wec</td>\n      <td>17</td>\n      <td>45</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>albane</td>\n      <td>4</td>\n      <td>29</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>paf</td>\n      <td>10</td>\n      <td>7</td>\n      <td>-3</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>homo</td>\n      <td>3</td>\n      <td>0</td>\n      <td>-3</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>kermess</td>\n      <td>4</td>\n      <td>1</td>\n      <td>-3</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>noël</td>\n      <td>7</td>\n      <td>4</td>\n      <td>-3</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>pc</td>\n      <td>5</td>\n      <td>1</td>\n      <td>-4</td>\n    </tr>\n  </tbody>\n</table>\n<p>143 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# conversion dictionnaire en list\n",
    "entities_with_sentiment_list = []\n",
    "for key in entities_with_sentiment:\n",
    "    temp = [key,entities_with_sentiment[key][0],entities_with_sentiment[key][1],entities_with_sentiment[key][2]]\n",
    "    entities_with_sentiment_list.append(temp)\n",
    "\n",
    "# conversion list en dataframe + trie\n",
    "df_senti = pd.DataFrame(entities_with_sentiment_list,columns=[\"entity\",\"negatif\",\"positif\",\"diff\"])\n",
    "\n",
    "df_senti = df_senti.sort_values(by=[\"diff\"],ascending=False)\n",
    "index = df_senti[df_senti[\"positif\"]+df_senti[\"negatif\"] <= 2].index\n",
    "df_senti = df_senti.drop(index)\n",
    "df_senti"
   ]
  },
  {
   "source": [
    "### Affichage des résultats"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2112   messags contenant une entitée (~30%)\n143    entitées apparaissant au moins 3 fois dans le corpus\n1958   associations (messages positifs, entitée)\n982    associations (messages négatifs, entitée)\n"
     ]
    }
   ],
   "source": [
    "msg_avec_entitees = len(messages_keep)-n_sans_entity\n",
    "print(f\"{msg_avec_entitees}   messags contenant une entitée (~{round(100*msg_avec_entitees/len(messages_keep))}%)\")\n",
    "print(f\"{len(df_senti)}    entitées apparaissant au moins 3 fois dans le corpus\")\n",
    "arr = np.array(entities_with_sentiment_list)\n",
    "neg = np.sum([int(n) for n in arr[:,1]])\n",
    "pos = np.sum([int(p) for p in arr[:,2]])\n",
    "print(f\"{pos}   associations (messages positifs, entitée)\")\n",
    "print(f\"{neg}    associations (messages négatifs, entitée)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1                  julie  39 138  99           |++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n2                 soirée  19  56  37           |+++++++++++++++++++++++++++++\n3               juliette  13  43  30           |++++++++++++++++++++++++\n4                    wec  17  45  28           |++++++++++++++++++++++\n5                 albane   4  29  25           |++++++++++++++++++++\n6                 julien  22  47  25           |++++++++++++++++++++\n7                  simon   8  31  23           |++++++++++++++++++\n8                antoine  15  36  21           |++++++++++++++++\n9                  élise   7  28  21           |++++++++++++++++\n10                  paul   8  29  21           |++++++++++++++++\n11                projet   6  26  20           |++++++++++++++++\n12                google   8  28  20           |++++++++++++++++\n13               matthéo  12  32  20           |++++++++++++++++\n14                  zoom  25  44  19           |+++++++++++++++\n15                    cm  41  60  19           |+++++++++++++++\n16                 bière  18  37  19           |+++++++++++++++\n17                  pull  24  42  18           |++++++++++++++\n18              election   4  22  18           |++++++++++++++\n19                margot  14  31  17           |+++++++++++++\n20                romain  11  28  17           |+++++++++++++\n21              campagne   4  20  16           |++++++++++++\n22                    3a   6  22  16           |++++++++++++\n23                    ru   4  19  15           |++++++++++++\n24                 fanny  10  25  15           |++++++++++++\n25                  tess  17  30  13           |++++++++++\n26                  theo   7  19  12           |+++++++++\n27                lydia    2  14  12           |+++++++++\n28                   bde  28  40  12           |+++++++++\n29                  ensc  50  61  11           |++++++++\n30                 respo   9  20  11           |++++++++\n31                   léa   2  12  10           |++++++++\n32                   pii   2  12  10           |++++++++\n33                   wei  10  20  10           |++++++++\n34                  info   5  15  10           |++++++++\n35                    ia   1  11  10           |++++++++\n36                 vodka   2  11   9           |+++++++\n37                nico l  18  27   9           |+++++++\n38              bordeaux  11  20   9           |+++++++\n39                   bds  12  21   9           |+++++++\n40                marine   5  13   8           |++++++\n41             octoflix    8  16   8           |++++++\n42               discord   2  10   8           |++++++\n43                  vert  14  22   8           |++++++\n44               shotgun   4  11   7           |+++++\n45               sabaidi  11  18   7           |+++++\n46                tiktok   1   8   7           |+++++\n47              corentin   9  16   7           |+++++\n48          compte rendu   8  15   7           |+++++\n49                  hugo   1   7   6           |++++\n50        benoit leblanc  12  18   6           |++++\n51                   d3a   0   6   6           |++++\n52                   wes   0   6   6           |++++\n53                  rhum   1   7   6           |++++\n54                ricard   1   7   6           |++++\n55               turn up   1   7   6           |++++\n56               poulpas   3   8   5           |++++\n57               netflix   0   5   5           |++++\n58                    tp   7  12   5           |++++\n59             photoshop   0   4   4           |+++\n60              melchior   0   4   4           |+++\n61                   jma   4   8   4           |+++\n62               atelier   0   4   4           |+++\n63               calabar   2   6   4           |+++\n64                   v&b   0   4   4           |+++\n65                 apéro   4   8   4           |+++\n66               ricardo   0   4   4           |+++\n67             instagram   3   7   4           |+++\n68               salotti   6  10   4           |+++\n69                  gars  39  42   3           |++\n70                   bci   1   4   3           |++\n71                sylvia   0   3   3           |++\n72              vacances   8  11   3           |++\n73                  meuf  17  20   3           |++\n74              hourlier   0   3   3           |++\n75                wei&go   2   5   3           |++\n76              télégram   0   3   3           |++\n77                  théa   4   7   3           |++\n78                    dm   0   3   3           |++\n79                 maths   8  11   3           |++\n80                matlab   0   3   3           |++\n81            australien   0   3   3           |++\n82             handicapé   0   3   3           |++\n83               pesquet   8  11   3           |++\n84                violet   0   3   3           |++\n85               youtube   0   3   3           |++\n86                  evan   7  10   3           |++\n87                  emma   1   3   2           |+\n88               barthas   1   3   2           |+\n89             deliveroo   1   3   2           |+\n90                 foyer  16  18   2           |+\n91                amaury   1   3   2           |+\n92    victorien marchand   1   3   2           |+\n93                   bda   9  11   2           |+\n94               mélissa   3   5   2           |+\n95                   inp   2   4   2           |+\n96                 tom d  19  21   2           |+\n97                 axure   1   3   2           |+\n98              lespinet   1   3   2           |+\n99                    1a  17  19   2           |+\n100             octowood   1   2   1           |\n101               france   1   2   1           |\n102              justine   2   3   1           |\n103               audrey   1   2   1           |\n104               maelle   2   3   1           |\n105                alban   1   2   1           |\n106               paques   1   2   1           |\n107            respo com   1   2   1           |\n108                jauze   1   2   1           |\n109             tournage   1   2   1           |\n110            vaisselle   5   6   1           |\n111               arthur   1   2   1           |\n112                jaune   1   2   1           |\n113             facebook   3   4   1           |\n114              martin    2   2   0           |\n115              enseirb   2   2   0           |\n116               airbnb   6   6   0           |\n117                 wiwi   5   5   0           |\n118                  qcm   4   4   0           |\n119               nadége   8   8   0           |\n120                covid  11  11   0           |\n121              apoulpo   3   3   0           |\n122                   2a   6   6   0           |\n123                hippo   2   2   0           |\n124                 zack   2   1  -1           |\n125                sarah   2   1  -1           |\n126                gradi   6   5  -1           |\n127               edwige   2   1  -1           |\n128                 sese   3   2  -1           |\n129                rouge   4   3  -1           |\n130           poulpywood  10   9  -1           |\n131           chrystelle   2   1  -1           |\n132               maiwen   2   1  -1           |\n133               canada   4   2  -2          -|\n134               orange   5   3  -2          -|\n135              olivier   6   4  -2          -|\n136                 chat   3   1  -2          -|\n137              coralie   4   2  -2          -|\n138                 gala   7   4  -3         --|\n139                  paf  10   7  -3         --|\n140                 homo   3   0  -3         --|\n141              kermess   4   1  -3         --|\n142                 noël   7   4  -3         --|\n143                   pc   5   1  -4        ---|\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "array_senti = df_senti.to_numpy()\n",
    "maxdiff = array_senti[0][3]\n",
    "for row in array_senti:\n",
    "    spaces1 = 24-len(row[0])\n",
    "    nb_char = int(row[3]*80/maxdiff)\n",
    "    nb_moins = nb_plus = 0\n",
    "    if nb_char < 0:\n",
    "        nb_moins = -nb_char\n",
    "    else:\n",
    "        nb_plus = nb_char\n",
    "    spaces2 = 10-nb_moins\n",
    "    counts = [f\"{' '*(3-len(str(row[i])))}{row[i]}\" for i in range(1,4)]\n",
    "    print(f\"{n}{' '*(spaces1-len(str(n)))}{row[0]} {counts[0]} {counts[1]} {counts[2]} {' '*spaces2}{'-'*nb_moins}|{'+'*nb_plus}\")\n",
    "    n+=1"
   ]
  }
 ]
}